{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5Zex-ktX5ME"
   },
   "source": [
    "# Getting started \n",
    "## Importing a labeled dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6rdGGv8m-lz"
   },
   "outputs": [],
   "source": [
    "!pip3 install labelbox[data]\n",
    "import labelbox\n",
    "from labelbox.schema.ontology import OntologyBuilder, Tool, Classification,Option\n",
    "from labelbox.schema.annotation_import import MALPredictionImport\n",
    "from labelbox.data.serialization import NDJsonConverter\n",
    "from labelbox.schema.annotation_import import LabelImport\n",
    "from labelbox.schema.queue_mode import QueueMode\n",
    "from labelbox.schema.media_type import MediaType\n",
    "from labelbox import LabelingFrontend\n",
    "from labelbox.data.annotation_types import (\n",
    "    Label,\n",
    "    Point,\n",
    "    LabelList,\n",
    "    ImageData,\n",
    "    Rectangle,\n",
    "    ObjectAnnotation,\n",
    ")\n",
    "from labelbox.schema.data_row_metadata import (\n",
    "    DataRowMetadata,\n",
    "    DataRowMetadataField,\n",
    "    DeleteDataRowMetadata,\n",
    "    DataRowMetadataKind\n",
    ")\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiKZnB4YPqHj"
   },
   "outputs": [],
   "source": [
    "## Generic data download function\n",
    "def download_files(filemap):\n",
    "    path, uri = filemap    \n",
    "    ## Download data\n",
    "    if not os.path.exists(path):\n",
    "        r = requests.get(uri, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(path, 'wb') as f:\n",
    "                for chunk in r:\n",
    "                    f.write(chunk)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqJt-G_aS-Z-"
   },
   "source": [
    "# Setup Labelbox client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiwPkXe4nJrS"
   },
   "outputs": [],
   "source": [
    "## Generate API key: https://app.labelbox.com/account/api-keys\n",
    "LB_API_KEY = \"\"\n",
    "client = labelbox.Client(LB_API_KEY)\n",
    "\n",
    "DATA_ROWS = \"https://storage.googleapis.com/labelbox-datasets/VHR_geospatial/geospatial_datarows.json\"\n",
    "ANNOTATIONS = \"https://storage.googleapis.com/labelbox-datasets/VHR_geospatial/geospatial_annotations.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5kXlyspS5GR"
   },
   "source": [
    "# Download a public dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEXN2pyt3YZQ"
   },
   "outputs": [],
   "source": [
    "download_files((\"data_rows.json\", DATA_ROWS))\n",
    "download_files((\"annotations.json\", ANNOTATIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfWoTuKeUX8n"
   },
   "outputs": [],
   "source": [
    "with open('data_rows.json', 'r') as fp:\n",
    "    data_rows = json.load(fp)\n",
    "\n",
    "with open('annotations.json', 'r') as fp:\n",
    "    annotations = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-VfjDaRSYtk"
   },
   "source": [
    "# Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ldzBOurIn8Ff"
   },
   "outputs": [],
   "source": [
    "dataset = client.create_dataset(name=\"Geospatial vessel detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zewMUN9WScOv"
   },
   "source": [
    "# Import Data Rows with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7JnTZMQ6Mnb"
   },
   "outputs": [],
   "source": [
    "# Here is an example of adding two metadata fields to your Data Rows: a \"captureDateTime\" field with datetime value, and a \"tag\" field with string value\n",
    "metadata_ontology = client.get_data_row_metadata_ontology()\n",
    "datetime_schema_id = metadata_ontology.reserved_by_name[\"captureDateTime\"].uid\n",
    "tag_schema_id = metadata_ontology.reserved_by_name[\"tag\"].uid\n",
    "tag_items = [\"WorldView-1\", \"WorldView-2\", \"WorldView-3\", \"WorldView-4\"]\n",
    "\n",
    "for datarow in tqdm(data_rows):\n",
    "    dt = datetime.datetime.utcnow() + datetime.timedelta(days=random.random()*30) # this is random datetime value\n",
    "    tag_item = random.choice(tag_items) # this is a random tag value\n",
    "\n",
    "    # Option 1: Specify metadata with a list of DataRowMetadataField. This is the recommended option since it comes with validation for metadata fields.\n",
    "    metadata_fields = [\n",
    "                       DataRowMetadataField(schema_id=datetime_schema_id, value=dt), \n",
    "                       DataRowMetadataField(schema_id=tag_schema_id, value=tag_item)\n",
    "                       ]\n",
    "\n",
    "    # Option 2: Uncomment to try. Alternatively, you can specify the metadata fields with dictionary format without declaring the DataRowMetadataField objects. It is equivalent to Option 1.\n",
    "    # metadata_fields = [\n",
    "    #                    {\"schema_id\": datetime_schema_id, \"value\": dt}, \n",
    "    #                    {\"schema_id\": tag_schema_id, \"value\": tag_item}\n",
    "    #                    ]\n",
    "\n",
    "    datarow[\"metadata_fields\"] = metadata_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_CrtIGqUimL"
   },
   "outputs": [],
   "source": [
    "task = dataset.create_data_rows(data_rows)\n",
    "task.wait_till_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpF4vH4eUkPt"
   },
   "source": [
    "Examine a Data Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGdJZLYoUmKH"
   },
   "outputs": [],
   "source": [
    "datarow = next(dataset.data_rows())\n",
    "print(datarow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4VjfJKESeoi"
   },
   "source": [
    "# Setup a labeling project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PR2sWSL-L7b"
   },
   "outputs": [],
   "source": [
    "ontology = OntologyBuilder()\n",
    "\n",
    "for tool in annotations['categories']:\n",
    "  print(tool['name'])\n",
    "  ontology.add_tool(Tool(tool = Tool.Type.BBOX, name = tool['name']))\n",
    "\n",
    "ontology = client.create_ontology(\"Vessel detection ontology\", ontology.asdict())\n",
    "project = client.create_project(name=\"Vessel detection\", media_type=MediaType.Image)\n",
    "project.setup_editor(ontology)\n",
    "ontology_from_project = OntologyBuilder.from_project(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvq-ndx1HSDN"
   },
   "source": [
    "Prepare and queue batch of Data Rows to the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YARsP1NZHRWj"
   },
   "outputs": [],
   "source": [
    "data_rows = [dr.uid for dr in list(dataset.export_data_rows())]\n",
    "\n",
    "# Randomly select 200 Data Rows\n",
    "sampled_data_rows = random.sample(data_rows, 200)\n",
    "\n",
    "batch = project.create_batch(\n",
    "  \"Initial batch\", # name of the batch\n",
    "  sampled_data_rows, # list of Data Rows\n",
    "  1 # priority between 1-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkCFk7qUSu53"
   },
   "source": [
    "# Process ground truth annotations for import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmeFTCk7-Vrn"
   },
   "outputs": [],
   "source": [
    "queued_data_rows = project.export_queued_data_rows()\n",
    "ground_truth_list = LabelList()\n",
    "\n",
    "for datarow in queued_data_rows:\n",
    "  annotations_list = []\n",
    "  folder = datarow['externalId'].split(\"/\")[0]\n",
    "  id = datarow['externalId'].split(\"/\")[1]\n",
    "  if folder == \"positive_image_set\":\n",
    "    for image in annotations['images']:\n",
    "      if (image['file_name']==id):\n",
    "        for annotation in annotations['annotations']:\n",
    "          if annotation['image_id'] == image['id']:\n",
    "            bbox = annotation['bbox']\n",
    "            id = annotation['category_id'] - 1\n",
    "            class_name = ontology_from_project.tools[id].name\n",
    "            annotations_list.append(ObjectAnnotation(\n",
    "                name = class_name,\n",
    "                value = Rectangle(start = Point(x = bbox[0], y = bbox[1]), end = Point(x = bbox[2]+bbox[0], y = bbox[3]+bbox[1])),\n",
    "            ))\n",
    "  image = ImageData(uid = datarow['id'])\n",
    "  ground_truth_list.append(Label(data = image, annotations = annotations_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCDxiydHSzeN"
   },
   "source": [
    "# Import ground truth annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71WXNj6FB60-"
   },
   "outputs": [],
   "source": [
    "ground_truth_ndjson = list(NDJsonConverter.serialize(ground_truth_list))\n",
    "\n",
    "start_time = time.time()\n",
    "## Upload annotations\n",
    "upload_task = LabelImport.create_from_objects(client, project.uid, \"geospatial-import-job-1\", ground_truth_ndjson)\n",
    "print(upload_task)\n",
    "\n",
    "#Wait for upload to finish (Will take up to five minutes)\n",
    "upload_task.wait_until_done()\n",
    "print(upload_task.errors)\n",
    "print(\"--- Finished in %s mins ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEU2CTtlWRL_"
   },
   "outputs": [],
   "source": [
    "# queued_data_rows = [dr['id'] for dr in list(project.export_queued_data_rows())]\n",
    "# data_rows = [dr.uid for dr in list(dataset.export_data_rows())]\n",
    "# data_rows_not_queued = list(set(data_rows)- set(queued_data_rows))\n",
    "\n",
    "# # Randomly select 200 Data Rows\n",
    "# sampled_data_rows = random.sample(data_rows_not_queued, 200)\n",
    "\n",
    "# batch = project.create_batch(\n",
    "#   \"Second batch\", # name of the batch\n",
    "#   sampled_data_rows, # list of Data Rows\n",
    "#   5 # priority between 1-5\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Getting Started: Import a labeled dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
